---
layout:     post
title:      "GBDT和Xgboost"
subtitle:   "两个重要的模型"
date:       2018-02-28 15:12:00
author:     "AJW"
header-img: "img/tech.jpg"
tags:
    - 模型
    - 机器学习
    - 算法
---

GBDT和Xgboost可以说是目前比较流行的机器学习模型，在各大比赛中也是大放异彩，借着毕设以及天池智能制造比赛的机会，也深入了解了一下。

两者可以说都是Boosting算法的具体实现，而且十分相似，这里主要比较一下两个模型的不同点（主要讲回归），而对于模型的具体算法和推导过程，可以参考[陈天奇的文章](http://www.52cs.org/?p=429)以及[知乎的回答](https://www.zhihu.com/question/41354392)以及其中的[PPT](http://wepon.me/files/gbdt.pdf)介绍，个人感觉是说的比较清楚的两个，这里也从中摘录了一些图和段落过来。

Boosting是一种加法模型，模型最终的结果是多个基模型结果相加得到的，GBDT和Xgboost在这一点上是相同的，个人认为两者比较大的区别在于，**GBDT是在函数空间通过梯度下降法来求解，而Xgboost是在函数空间通过牛顿法来求解，同时Xgboost还加入了正则化项来进行bias-variance的tradeoff**。当然还有一些具体实现上的区别，这个可以具体看一下上面知乎的回答。下面分别来看一下这两种算法。

### GBDT

GBDT的算法流程如下图所示

![GBDT算法流程](/img/in-post/GBDT/GBDT.PNG)

这里需要解释一下的是，各种教程上都说，后续树的构建都是要拟合“残差”，但是为啥图中的流程就变成损失函数的负梯度了？其实我不是很喜欢从“残差”这个角度去解释，而是从函数空间的角度去想。在参数空间内，梯度下降大家应该都很熟悉了，只需要不断将参数向着损失函数的负梯度方向调整就可以了，也就是说，最后的参数值或者说自变量的值，可以写作$$x = x_0 + \sum_{m=0}^Mx_m$$,其中，$x_m = -\rho_mg_m$,$g_m$ 是损失函数的梯度方向，也就是从初值开始，不断进行负梯度方向的累加，直到达到最优值。

而GBDT的优化其实是在函数空间的，参数空间内的x变成了一个函数（或者说是一棵树），所以，我们树的输出也要向着损失函数的负梯度方向调整，下一棵树也就要拟合损失函数的负梯度。

这个思路是论文中的思路，可以查看Friedman的论文Greedy Function Approximation：A Gradient Boosing Machine



### Xgboost

Xgboost的基本模型和GBDT还是很像的，区别在于，Xgboost在损失函数优化时，采用了牛顿法，也就是取了二阶的泰勒展开，包含了损失函数的二阶导数信息。同时还在损失函数中加入了正则项。

![GBDT算法流程](/img/in-post/GBDT/Xgboost1.PNG)

![GBDT算法流程](/img/in-post/GBDT/Xgboost2.PNG)

![GBDT算法流程](/img/in-post/GBDT/Xgboost3.PNG)

![GBDT算法流程](/img/in-post/GBDT/Xgboost4.PNG)

在建树的过程中，Xgboost只需要计算分裂前后损失函数的增益，选择最大的特征进行分裂。

另外，Xgboost的可并行，并不是说树的训练可以并行，而是说特征粒度的并行，也就是说不同特征分裂选择上可以选择并行。（这里其实我不是很明白，难道GBDT不可以并行？个人感觉这个应该只是实现上的一种优化吧）



总的来说，**从参数空间到函数空间**的变化对于理解GBDT和Xgboot来说是比较重要的，我认为也是想要深入理解这两种方法的正确途径。